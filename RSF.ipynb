{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Description and Related Work <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business cycle describes the rise and fall in the growth of the economy that occurs over time. Each business cyle has two turning points trough (or bottom) and peak. Expansion is measured from the trough of the previous business cycle to the peak of the current cycle, while recession is measured from the peak to the trough. In the United States (US), the Business Cycle Dating Committee of the National Bureau of Economic Research (NBER) determines the dates for business cycles. The turning points are determined by considering monthly growth indicators of the economy such as industrial production, employment, real income. The main focus of business cycle analysis is to analyze why economy goes through contraction and expansion periods. It is a well-studied topic in the literature and still an active reserach area. In our work, we are going to predict recessions in US economy with leading macroeconomic indicators using machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understand Data <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Set Up Environment <a id=\"2.0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read Data and Description of the Variables <a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis we will use a large macroeconomic database from FRED St. Louis designed by [McCracken and Ng (2015)](#i). It involves 129 macroeconomic monthly time series over the period 1959-2018. The data is organized into 8 categories (1)output and income, (2)labor market, (3)housing, (4)consumption, orders and inventories, (5)money and credit, (6)interest and exchange rates, (7)prices and (8)stock market. Detail description of the variables under each category can be found in __[appendix]( https://s3.amazonaws.com/files.fred.stlouisfed.org/fred-md/Appendix_Tables_Update.pdf)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmacro=pd.read_csv(\"fred-md-current.csv\")\n",
    "bigmacro=bigmacro.rename(columns={'sasdate':'Date'})\n",
    "bigmacro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with the previous works in the literature, we use __[business cycle dating chronology provided by NBER](http://www.nber.org/cycles.html)__  which involves dates when recession began and ended in US economy. According to NBER's statistics we have 8 recession periods in our dataset where duration is changing from 6 to 18 months. We represent regimes as \"Normal\" and \"Recession\" in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recession_periods=pd.read_csv('Recession_Periods.csv')\n",
    "bigmacro.insert(loc=1,column=\"Regime\", value=Recession_periods['Regime'].values)\n",
    "bigmacro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmacro[[\"Date\",\"Regime\"]].groupby(\"Regime\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of the dataset for recession forecasting. Cleaning of the data and feature selection to reduce the number of variables in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Up Environment and Read Data <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller #to check unit root in time series \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import seaborn as sns #for correlation heatmap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmacro=pd.read_csv(\"Macroeconomic_Variables.csv\")\n",
    "bigmacro=bigmacro.rename(columns={'sasdate':'Date'})\n",
    "Recession_periods=pd.read_csv('Recession_Periods.csv')\n",
    "bigmacro.insert(loc=1,column=\"Regime\", value=Recession_periods['Regime'].values)\n",
    "bigmacro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the steps below to clean data and make it ready for feature selection process.\n",
    "\n",
    "1. Remove the variables with missing observations\n",
    "2. Add lags of the variables as additional features\n",
    "3. Test stationarity of time series\n",
    "4. Standardize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns with missing observations\n",
    "missing_colnames=[]\n",
    "for i in bigmacro.drop(['Date','Regime'],axis=1):\n",
    "    observations=len(bigmacro)-bigmacro[i].count()\n",
    "    if (observations>10):\n",
    "        print(i+':'+str(observations))\n",
    "        missing_colnames.append(i)\n",
    " \n",
    "bigmacro=bigmacro.drop(labels=missing_colnames, axis=1)\n",
    "\n",
    "#rows with missing values\n",
    "bigmacro=bigmacro.dropna(axis=0)\n",
    "\n",
    "bigmacro.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lags\n",
    "for col in bigmacro.drop(['Date', 'Regime'], axis=1):\n",
    "    for n in [3,6,9,12,18]:\n",
    "        bigmacro['{} {}M lag'.format(col, n)] = bigmacro[col].shift(n).ffill().values\n",
    "\n",
    "# 1 month ahead prediction\n",
    "bigmacro[\"Regime\"]=bigmacro[\"Regime\"].shift(-1)\n",
    "\n",
    "bigmacro=bigmacro.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmacro.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented Dickey-Fuller Test can be used to test for stationarity in macroeconomic time series variables. We will use `adfuller` function from `statsmodels` module in Python. More information about the function can be found __[here](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check stationarity\n",
    "from statsmodels.tsa.stattools import adfuller #to check unit root in time series \n",
    "threshold=0.01 #significance level\n",
    "for column in bigmacro.drop(['Date','Regime'], axis=1):\n",
    "    result=adfuller(bigmacro[column])\n",
    "    if result[1]>threshold:\n",
    "        bigmacro[column]=bigmacro[column].diff()\n",
    "bigmacro=bigmacro.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.01 #significance level\n",
    "for column in bigmacro.drop(['Date','Regime'], axis=1):\n",
    "    result=adfuller(bigmacro[column])\n",
    "    if result[1]>threshold:\n",
    "        bigmacro[column]=bigmacro[column].diff()\n",
    "bigmacro=bigmacro.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.01 #significance level\n",
    "for column in bigmacro.drop(['Date','Regime'], axis=1):\n",
    "    result=adfuller(bigmacro[column])\n",
    "    if result[1]>threshold:\n",
    "        print(column)\n",
    "bigmacro=bigmacro.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features=bigmacro.drop(['Date','Regime'],axis=1)\n",
    "col_names=features.columns\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(features)\n",
    "standardized_features=scaler.transform(features)\n",
    "standardized_features.shape\n",
    "df=pd.DataFrame(data=standardized_features,columns=col_names)\n",
    "df.insert(loc=0,column=\"Date\", value=bigmacro['Date'].values)\n",
    "df.insert(loc=1,column='Regime', value=bigmacro['Regime'].values)\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Dataset_Cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Up Environment and Read Data <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from matplotlib import pyplot as mp\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Dataset_Cleaned.csv')\n",
    "\n",
    "Label = df[\"Regime\"].apply(lambda regime: 1. if regime == 'Normal' else 0.)\n",
    "df.insert(loc=2, column=\"Label\", value=Label.values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our exercise will be based on classification problem. We have two binary outcomes that we want to predict with certain variables. Here we will summarize our approach to predict recessions with machine learning algorithms.\n",
    "\n",
    "1. We will perform feature selection before making our forecasts. We will use $L_1$ regularized logistic regression for that purpose.\n",
    "\n",
    "2. Separate dataset into training and validation datasets. Split based dataset based on time: the period over 1960-1996 is selected for training and the period over 1996-2018 is kept for validation\n",
    "\n",
    "3. Evaluate performances of the machine learning algorithms on training dataset with cross validation (CV). Since we have time series structure we will use a special type of CV function in Python,__[`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit)__. We will use Receiver operating characteristic (ROC) as scoring metric in our models. Related Python functions for this metric are __[`roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)__  and  __[`roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)__.\n",
    "\n",
    "4. Select the best performing models based on average accuracy and standard deviation of the CV results. We will take logistic regression as a benchmark model since this is the traditional method has been used to approach this problem.\n",
    "\n",
    "5. Then we make predictions on the validation dataset with selected models. First, we use __[`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)__ for selected model on training dataset to find best combination of parameters for the model. Then we evaluate the model on validation dataset and report accuracy metrics and feature importance results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection with $L_1$ Penalty <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Split\n",
    "df_idx = df[df.Date == '12/1/96'].index[0]\n",
    "\n",
    "df_targets=df['Label'].values\n",
    "df_features=df.drop(['Regime','Date','Label'], axis=1)\n",
    "\n",
    "df_training_features = df.iloc[:df_idx,:].drop(['Regime','Date','Label'], axis=1)\n",
    "df_validation_features = df.iloc[df_idx:, :].drop(['Regime','Date','Label'], axis=1)\n",
    "\n",
    "df_training_targets = df['Label'].values\n",
    "df_training_targets=df_training_targets[:df_idx]\n",
    "\n",
    "df_validation_targets = df['Label'].values\n",
    "df_validation_targets=df_validation_targets[df_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_training_features),len(df_training_targets),len(df_targets))\n",
    "print(len(df_validation_features),len(df_validation_targets),len(df_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring=\"roc_auc\"\n",
    "kfold= model_selection.TimeSeriesSplit(n_splits=3)\n",
    "seed=8\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.reciprocal([0.00000001, 0.00000005, 0.0000001, 0.0000005, 0.000001, 0.000005, 0.00001, 0.00005, \n",
    "                         0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000])\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C)\n",
    "\n",
    "model=LogisticRegression(max_iter=10000,penalty='l1')\n",
    "LR_penalty=model_selection.GridSearchCV(estimator=model, param_grid= hyperparameters,\n",
    "                                        cv=kfold, scoring=scoring).fit(X=df_features,\n",
    "                                                                       y=df_targets).best_estimator_\n",
    "\n",
    "LR_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_features\n",
    "y=df_targets\n",
    "lr_l1 = LogisticRegression(C=0.1, max_iter=10000,penalty=\"l1\").fit(X,y)\n",
    "model = SelectFromModel(lr_l1,prefit=True)\n",
    "feature_idx = model.get_support()\n",
    "feature_name = X.columns[feature_idx]\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2=df[feature_name]\n",
    "df_2.insert(loc=0, column=\"Date\", value=df['Date'].values)\n",
    "df_2.insert(loc=1, column=\"Regime\", value=df['Regime'].values)\n",
    "df_2.insert(loc=2, column=\"Label\", value=df['Label'].values)\n",
    "\n",
    "df_2.head()\n",
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_2.drop(['Date','Regime','Label'],axis=1).corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), \n",
    "            cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Algorithms on Training Dataset <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_2\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Split\n",
    "df_idx = df[df.Date == '12/1/96'].index[0]\n",
    "\n",
    "df_targets=df['Label'].values\n",
    "df_features=df.drop(['Regime','Date','Label'], axis=1)\n",
    "\n",
    "df_training_features = df.iloc[:df_idx,:].drop(['Regime','Date','Label'], axis=1)\n",
    "df_validation_features = df.iloc[df_idx:, :].drop(['Regime','Date','Label'], axis=1)\n",
    "\n",
    "df_training_targets = df['Label'].values\n",
    "df_training_targets=df_training_targets[:df_idx]\n",
    "\n",
    "df_validation_targets = df['Label'].values\n",
    "df_validation_targets=df_validation_targets[df_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=8\n",
    "scoring='roc_auc' \n",
    "kfold = model_selection.TimeSeriesSplit(n_splits=3) \n",
    "models = []\n",
    "\n",
    "models.append(('LR', LogisticRegression(C=1e09)))\n",
    "models.append(('LR_L1', LogisticRegression(penalty = 'l1')))\n",
    "models.append(('LR_L2', LogisticRegression(penalty = 'l2')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('GB', GradientBoostingClassifier()))\n",
    "models.append(('ABC', AdaBoostClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('XGB', xgb.XGBClassifier()))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "for name, model in models:\n",
    "    cv_results = model_selection.cross_val_score(estimator = model, X = df_training_features, \n",
    "                                                 y = lb.fit_transform(df_training_targets), cv=kfold, scoring = scoring)\n",
    "    \n",
    "    model.fit(df_training_features, df_training_targets) # train the model\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(df_training_targets, model.predict_proba(df_training_features)[:,1])\n",
    "    auc = metrics.roc_auc_score(df_training_targets,model.predict(df_training_features))\n",
    "    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (name, auc))\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show() \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison based on Cross Validation Scores')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Performances of the Algorithms on Validation Dataset <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(C=1e09) # high penalty \n",
    "LR=model.fit(df_training_features,df_training_targets)\n",
    "training_predictions=LR.predict(df_training_features)\n",
    "prob_predictions = LR.predict_proba(df_training_features)\n",
    "prob_predictions = np.append(prob_predictions, LR.predict_proba(df_validation_features), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# define periods of recession\n",
    "rec_spans = []\n",
    "#rec_spans.append([datetime.datetime(1957,8,1), datetime.datetime(1958,4,1)])\n",
    "rec_spans.append([datetime.datetime(1960,4,1), datetime.datetime(1961,2,1)])\n",
    "rec_spans.append([datetime.datetime(1969,12,1), datetime.datetime(1970,11,1)])\n",
    "rec_spans.append([datetime.datetime(1973,11,1), datetime.datetime(1975,3,1)])\n",
    "rec_spans.append([datetime.datetime(1980,1,1), datetime.datetime(1980,6,1)])\n",
    "rec_spans.append([datetime.datetime(1981,7,1), datetime.datetime(1982,10,1)])\n",
    "rec_spans.append([datetime.datetime(1990,7,1), datetime.datetime(1991,2,1)])\n",
    "rec_spans.append([datetime.datetime(2001,3,1), datetime.datetime(2001,10,1)])\n",
    "rec_spans.append([datetime.datetime(2007,12,1), datetime.datetime(2009,5,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_range = pd.date_range(start='9/1/1960', end='9/1/2018', freq='MS')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(sample_range.to_series().values, prob_predictions[:,0])\n",
    "for i in range(len(rec_spans)):\n",
    "    plt.axvspan(rec_spans[i][0], rec_spans[i][len(rec_spans[i]) - 1], alpha=0.25, color='grey')\n",
    "plt.axhline(y=0.5, color='r', ls='dashed', alpha = 0.5)\n",
    "plt.title('Recession Prediction Probabalities with Logistic Regression')\n",
    "mp.savefig('plot1.png',  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.reciprocal([0.00000001, 0.00000005, 0.0000001, 0.0000005, 0.000001, 0.000005, 0.00001, 0.00005, \n",
    "                         0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000])\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "\n",
    "model=LogisticRegression(max_iter=10000)\n",
    "LR_penalty=model_selection.GridSearchCV(estimator=model, param_grid= hyperparameters,\n",
    "                                        cv=kfold, scoring=scoring).fit(df_training_features,\n",
    "                                                                       df_training_targets).best_estimator_\n",
    "training_predictions=LR_penalty.predict(df_training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_predictions = LR_penalty.predict_proba(df_training_features)\n",
    "prob_predictions = np.append(prob_predictions, LR_penalty.predict_proba(df_validation_features), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_range = pd.date_range(start='9/1/1960', end='9/1/2018', freq='MS')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(sample_range.to_series().values, prob_predictions[:,0])\n",
    "for i in range(len(rec_spans)):\n",
    "    plt.axvspan(rec_spans[i][0], rec_spans[i][len(rec_spans[i]) - 1], alpha=0.25, color='grey')\n",
    "plt.axhline(y=0.5, color='r', ls='dashed', alpha = 0.5)\n",
    "plt.title('Recession Prediction Probabalities with Regularized Logistic Regression')\n",
    "mp.savefig('plot2.png',  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = model_selection.GridSearchCV(estimator=xgb.XGBClassifier(),\n",
    "                                       param_grid={'booster': ['gbtree']},\n",
    "                                       scoring=scoring, cv=kfold).fit(df_training_features, \n",
    "                                                                      lb.fit_transform(df_training_targets)).best_estimator_\n",
    "xgboost.fit(df_training_features, df_training_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_predictions = xgboost.predict_proba(df_training_features)\n",
    "prob_predictions = np.append(prob_predictions, xgboost.predict_proba(df_validation_features), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_range = pd.date_range(start='9/1/1960', end='9/1/2018', freq='MS')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(sample_range.to_series().values, prob_predictions[:,0])\n",
    "for i in range(len(rec_spans)):\n",
    "    plt.axvspan(rec_spans[i][0], rec_spans[i][len(rec_spans[i]) - 1], alpha=0.25, color='grey')\n",
    "plt.axhline(y=0.5, color='r', ls='dashed', alpha = 0.5)\n",
    "plt.title('Recession Prediction Probabalities with XGBoost')\n",
    "mp.savefig('plot3.png',  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find feature importances\n",
    "headers = df.drop(['Regime','Label', 'Date'], axis=1).columns.values.tolist()\n",
    "xgboost_importances = pd.DataFrame(xgboost.feature_importances_, index = headers, columns = ['Relative Importance'])\n",
    "_ = xgboost_importances.sort_values(by = ['Relative Importance'], ascending = False, inplace=True)\n",
    "xgboost_importances = xgboost_importances[xgboost_importances['Relative Importance']>0].iloc[:20]\n",
    "\n",
    "# display importances in bar-chart and pie-chart\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.xticks(rotation='90')\n",
    "plt.barh(y=np.arange(len(xgboost_importances)), width=xgboost_importances['Relative Importance'], align='center', tick_label=xgboost_importances.index)\n",
    "plt.gca().invert_yaxis()\n",
    "mp.savefig('feature_importance.png',  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(df_validation_targets, LR.predict_proba(df_validation_features)[:,1])\n",
    "auc = metrics.roc_auc_score(df_validation_targets,LR.predict(df_validation_features))\n",
    "plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % ('LR', auc))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(df_validation_targets, LR_penalty.predict_proba(df_validation_features)[:,1])\n",
    "auc = metrics.roc_auc_score(df_validation_targets,LR_penalty.predict(df_validation_features))\n",
    "plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % ('LR_penalty', auc))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(df_validation_targets, xgboost.predict_proba(df_validation_features)[:,1])\n",
    "auc = metrics.roc_auc_score(df_validation_targets,xgboost.predict(df_validation_features))\n",
    "plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % ('XGBoost', auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic (Validation Data)')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "mp.savefig('ROC1.png',  bbox_inches='tight')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(df_targets, LR.predict_proba(df_features)[:,1])\n",
    "auc = metrics.roc_auc_score(df_targets,LR.predict(df_features))\n",
    "plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % ('LR', auc))\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(df_targets, LR_penalty.predict_proba(df_features)[:,1])\n",
    "auc = metrics.roc_auc_score(df_targets,LR_penalty.predict(df_features))\n",
    "plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % ('LR_penalty', auc))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(df_targets, xgboost.predict_proba(df_features)[:,1])\n",
    "auc = metrics.roc_auc_score(df_targets,xgboost.predict(df_features))\n",
    "plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % ('XGBoost', auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic (Whole period)')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "mp.savefig('ROC2.png',  bbox_inches='tight')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
